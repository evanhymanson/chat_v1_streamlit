{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.llms.anthropic import Anthropic\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import Settings\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import MarkdownElementNodeParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# API access to llama-cloud\n",
    "LLAMA_CLOUD_API_KEY = os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
    "# Using Anthropic API for embeddings/LLMs\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "llm = Anthropic(model=\"claude-3-opus-20240229\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = llm\n",
    "Settings.embed_model = \"local:BAAI/bge-small-en-v1.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 71fa1fdb-eeec-4897-9974-bc73b4d04eab\n",
      ".........Started parsing the file under job_id e2944c8f-c598-4219-9353-b5cf1ed2410c\n"
     ]
    }
   ],
   "source": [
    "# Parse \n",
    "parser = LlamaParse(\n",
    "    api_key= LLAMA_CLOUD_API_KEY,  # can also be set in your env as LLAMA_CLOUD_API_KEY\n",
    "    result_type=\"markdown\",  # \"markdown\" and \"text\" are available\n",
    "    parsing_instruction=\"\"\"It contains specifications for different components. \n",
    "                            Reconstruct the information in a concise way.\"\"\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "file_extractor = {\".pdf\": parser}\n",
    "documents = SimpleDirectoryReader(\"./data_pistons\", file_extractor=file_extractor).load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "166it [00:00, 55262.68it/s]\n",
      "  0%|          | 0/166 [00:00<?, ?it/s]/Users/evanhymanson/Desktop/LlamaIndex/venv/lib/python3.12/site-packages/llama_index/core/program/utils.py:58: UserWarning: Failed to use `OpenAIPydanticProgram`. Please ensure that is installed by running `pip install llama-index-program-openai`.\n",
      "  warnings.warn(\n",
      "/Users/evanhymanson/Desktop/LlamaIndex/venv/lib/python3.12/site-packages/llama_index/core/program/utils.py:58: UserWarning: Failed to use `OpenAIPydanticProgram`. Please ensure that is installed by running `pip install llama-index-program-openai`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 166/166 [07:46<00:00,  2.81s/it]\n",
      "2it [00:00, 32263.88it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]/Users/evanhymanson/Desktop/LlamaIndex/venv/lib/python3.12/site-packages/llama_index/core/program/utils.py:58: UserWarning: Failed to use `OpenAIPydanticProgram`. Please ensure that is installed by running `pip install llama-index-program-openai`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 2/2 [00:12<00:00,  6.10s/it]\n"
     ]
    }
   ],
   "source": [
    "# Nodes\n",
    "\n",
    "node_parser = MarkdownElementNodeParser(llm=llm, num_workers=8)\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "base_nodes, objects = node_parser.get_nodes_and_objects(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reranker: most relevant docs\n",
    "from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker\n",
    "reranker = FlagEmbeddingReranker(top_n=5, model=\"BAAI/bge-reranker-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Query Engine: for reading tables too \n",
    "\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.indices.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "recursive_index = VectorStoreIndex(nodes=base_nodes+objects)\n",
    "index = recursive_index\n",
    "recursive_query_engine = recursive_index.as_query_engine(similarity_top_k=4, \n",
    "    node_postprocessors=[reranker], \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For printing response \n",
    "from llama_index.core.response.pprint_utils import pprint_response\n",
    "from llama_index.core import StorageContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query \n",
    "#query = \"Our client Audi wants to build a piston for their engine. Can you give me some recommendations? Here are the provided specifications: 1. ⁠Bore Diameter- 83.50 mm 2.⁠ ⁠Compression Height- 29.59 mm (1.165 inches) 3.⁠ ⁠Material- Forged 2618 T6 Aluminum 4.⁠ ⁠Compression Ratio- 8.5:1 5.⁠ ⁠Maximum G-Force- 8000 G's.\"\n",
    "#response = recursive_query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import MetadataMode\n",
    "from llama_index.core.response.pprint_utils import pprint_response\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "#pprint_response(response, show_source=True)\n",
    "\n",
    "#print(response)\n",
    "\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=3900)\n",
    "\n",
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode=\"condense_plus_context\",\n",
    "    memory=memory,\n",
    "    llm=llm,\n",
    "    context_prompt=(\n",
    "        \"You are a manufacturing assistant. All numerical data that you encounter are specifications for components. You need to give recommendations of components that have the least numerical difference in specifications to the specifications provided by the user. The specifications of recommendations should also semantically be the most similar to the specifications provided by the user. \"\n",
    "        \"Here are the relevant documents for the context:\\n\"\n",
    "        \"{context_str}\"\n",
    "        \"\\nInstruction: Use the previous chat history, or the context above, to interact and help the user.\"\n",
    "    ),\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = chat_engine.chat(\"Our client Audi wants to build a piston for their engine. Can you give me some recommendations? Here are the provided specifications: 1.⁠ ⁠Bore Diameter- 83.50 mm 2.⁠ ⁠Compression Height- 29.59 mm (1.165 inches) 3.⁠ ⁠Material- Forged 2618 T6 Aluminum 4.⁠ ⁠Compression Ratio- 8.5:1 5.⁠ ⁠Maximum G-Force- 8000 G's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the specifications provided by Audi, I would recommend the following piston kits that have the closest matching specifications:\n",
      "\n",
      "1. K450X3903 \n",
      "- Bore: 3.903\" (99.14 mm) - slightly larger than Audi's 83.50 mm bore\n",
      "- Compression Ratio: 9.8 - 10.3 depending on combustion chamber volume. Closest to Audi's 8.5:1 spec.\n",
      "- Pin Part #: 3905GFX (rated for 8000 G's max)\n",
      "\n",
      "2. K0103A3\n",
      "- Bore: 4.03\" (102.36 mm) \n",
      "- Stroke: 3.5\"\n",
      "- Compression Ratio: 8.9 - 9.2. Very close to Audi's 8.5:1 spec.\n",
      "- Pin Part #: W100F8-4040-5 \n",
      "- Rings: Strutted\n",
      "\n",
      "3. K0104A4\n",
      "- Bore: 4.04\" (102.62 mm)\n",
      "- Stroke: 4.0\" \n",
      "- Compression Ratio: 8.9 - 9.2\n",
      "- Pin Part #: W100F8-4040-5\n",
      "- Rings: Strutted\n",
      "\n",
      "A few notes:\n",
      "- The catalog doesn't specify materials, but these high performance kits are likely forged aluminum. You'd need to confirm 2618 T6 specifically.\n",
      "- Compression height of 1.165\" (29.59mm) is not listed for any kit. The closest is 1.050\" for a couple 4.005\" bore kits. Custom pistons may be needed to match this height exactly.\n",
      "- Bore sizes are slightly larger than Audi's 83.5mm (3.287\"). Custom boring to 83.5mm may be possible.\n",
      "- The pins are rated for 8000 G's maximum to meet Audi's spec.\n",
      "\n",
      "In summary, the K0103A3 and K0104A4 kits have compression ratios of 8.9-9.2 which are the closest match to Audi's 8.5:1 spec. The K450X3\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node, metadata for final response \n",
    "print(response.source_nodes[0].metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
