{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# API access to llama-cloud\n",
    "LLAMA_CLOUD_API_KEY = os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
    "# Using Anthropic API for embeddings/LLMs\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-3.5-Turbo Setup\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# for the purpose of this example, we will use the small model embedding and gpt3.5\n",
    "embed_model=OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature= 0.5, system_prompt=\"You are a manufacturing assistant. All numerical data that you encounter are specifications for components. You need to give recommendations of components that have the least numerical difference in specifications to the specifications provided by the user. The specifications of recommendations should also semantically be the most similar to the specifications provided by the user. \")\n",
    "\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 7a76d433-2e64-45c9-bf8c-03954d484cd5\n",
      "Started parsing the file under job_id 169a0b21-2439-4a2e-bf90-1fe1a61738de\n"
     ]
    }
   ],
   "source": [
    "# Parsing with instructions \n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "parser = LlamaParse(\n",
    "    api_key= LLAMA_CLOUD_API_KEY,  # can also be set in your env as LLAMA_CLOUD_API_KEY\n",
    "    result_type=\"markdown\",  # \"markdown\" and \"text\" are available\n",
    "    parsing_instruction=\"\"\"It contains tables and specifications for different components. \n",
    "                            Try to reconstruct the information in a concise way.\"\"\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "file_extractor = {\".pdf\": parser}\n",
    "documents = SimpleDirectoryReader(\"./data_pistons\", file_extractor=file_extractor).load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85it [00:00, 65128.94it/s]\n",
      "100%|██████████| 85/85 [00:30<00:00,  2.75it/s]\n",
      "9it [00:00, 62914.56it/s]\n",
      "100%|██████████| 9/9 [00:03<00:00,  2.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# Nodes\n",
    "from llama_index.core.node_parser import MarkdownElementNodeParser\n",
    "\n",
    "node_parser = MarkdownElementNodeParser(llm=OpenAI(model=\"gpt-3.5-turbo-0125\"), num_workers=8)\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "base_nodes, objects = node_parser.get_nodes_and_objects(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reranker: most relevant docs\n",
    "from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker\n",
    "reranker = FlagEmbeddingReranker(top_n=5, model=\"BAAI/bge-reranker-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Query Engine: for reading tables too \n",
    "\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.indices.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "recursive_index = VectorStoreIndex(nodes=base_nodes+objects)\n",
    "index = recursive_index\n",
    "recursive_query_engine = recursive_index.as_query_engine(similarity_top_k=4, \n",
    "    node_postprocessors=[reranker], \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For printing response \n",
    "from llama_index.core.response.pprint_utils import pprint_response\n",
    "from llama_index.core import StorageContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query \n",
    "#query = \"Our client Audi wants to build a piston for their engine. Can you give me some recommendations? Here are the provided specifications: 1. ⁠Bore Diameter- 83.50 mm 2.⁠ ⁠Compression Height- 29.59 mm (1.165 inches) 3.⁠ ⁠Material- Forged 2618 T6 Aluminum 4.⁠ ⁠Compression Ratio- 8.5:1 5.⁠ ⁠Maximum G-Force- 8000 G's.\"\n",
    "#response = recursive_query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import MetadataMode\n",
    "from llama_index.core.response.pprint_utils import pprint_response\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "#pprint_response(response, show_source=True)\n",
    "\n",
    "#print(response)\n",
    "\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=3900)\n",
    "\n",
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode=\"condense_plus_context\",\n",
    "    memory=memory,\n",
    "    llm=llm,\n",
    "    context_prompt=(\n",
    "        \"You are a manufacturing assistant. All numerical data that you encounter are specifications for components. You need to give recommendations of components that have the least numerical difference in specifications to the specifications provided by the user. The specifications of recommendations should also semantically be the most similar to the specifications provided by the user. \"\n",
    "        \"Here are the relevant documents for the context:\\n\"\n",
    "        \"{context_str}\"\n",
    "        \"\\nInstruction: Use the previous chat history, or the context above, to interact and help the user.\"\n",
    "    ),\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = chat_engine.chat(\"Our client Audi wants to build a piston for their engine. Can you give me some recommendations? Here are the provided specifications: 1.⁠ ⁠Bore Diameter- 83.50 mm 2.⁠ ⁠Compression Height- 29.59 mm (1.165 inches) 3.⁠ ⁠Material- Forged 2618 T6 Aluminum 4.⁠ ⁠Compression Ratio- 8.5:1 5.⁠ ⁠Maximum G-Force- 8000 G's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the specifications provided for the Audi engine piston, here are some recommendations for components that closely match the criteria:\n",
      "\n",
      "1. **Kit Part #:** KE201M825\n",
      "2. **Bore mm / Inch:** 82.50 / 3.248\n",
      "3. **Over Sizes:** 1.50\n",
      "4. **Stroke:** 6.260\n",
      "5. **Rod Lgth.:** 1.204\n",
      "6. **Comp Ht.:** 32.65\n",
      "7. **Head cc’s:** 46\n",
      "8. **Dome Dish:** -19.4\n",
      "9. **Gasket Thickness:** 0.065\n",
      "10. **Deck Clearance:** 0.000\n",
      "11. **Block Ht.:** 9.291\n",
      "12. **Comp. Ratio:** 8.0:1\n",
      "13. **Pin Part # (included):** 8300XX\n",
      "14. **Ring # (included):** 308\n",
      "15. **Gram Foot:** 1\n",
      "\n",
      "This piston closely matches the specifications provided, with a bore size of 82.50 mm, a compression height of 29.59 mm, and a compression ratio of 8.0:1. The material used is forged 2618 T6 aluminum, which meets the client's requirements.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node, metadata for final response \n",
    "print(response.source_nodes[0].metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
